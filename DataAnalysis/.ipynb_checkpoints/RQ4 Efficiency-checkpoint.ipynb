{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Efficiency\n",
    "We measure the efficiency of E-Test in terms of following metrics:\n",
    "- [LLM prompt time](#llm-prompt-time)\n",
    "- [RAG index time](#rag-index-time)\n",
    "- [Consumed tokens](#consumed-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import huggingface_hub\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a065afb1e5425dab4d3928dbd0ed7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for plots\n",
    "# plt.rcParams.update({\"text.usetex\": True, \"font.family\": \"Times New Roman\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-695a6e12-39b735cf2153f2854e7f4695;7a35d12a-9c72-4806-bc46-9e7a638e356a)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:402\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1114\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1655\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1652\u001b[0m ):\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1460\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:283\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:307\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:419\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    416\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-695a6e12-39b735cf2153f2854e7f4695;7a35d12a-9c72-4806-bc46-9e7a638e356a)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m      2\u001b[0m RAW_EXPERIMENT_MODELS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT3.5Turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFineTunedGPT3.5Turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3 70B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m RAG_EXPERIMENT_MODELS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT3.5Turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFineTunedGPT3.5Turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3.3 70B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n\u001b[1;32m     16\u001b[0m TOKENIZER_MAP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m: tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize,\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m PLOT_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplotdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m TEST_SUITE_STATS_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplotdata/test_suite_stats.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:901\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1075\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1073\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1075\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1077\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:594\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:653\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformers/utils/hub.py:360\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-695a6e12-39b735cf2153f2854e7f4695;7a35d12a-9c72-4806-bc46-9e7a638e356a)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "EXPERIMENT_RESULTS_PATH = \"../AutonomicTester/experiment_results\"\n",
    "RAW_EXPERIMENT_MODELS = [\n",
    "    \"GPT3.5Turbo\",\n",
    "    \"FineTunedGPT3.5Turbo\",\n",
    "    \"GPT4Turbo\",\n",
    "    \"Llama3 8B\",\n",
    "    \"Llama3 70B\",\n",
    "]\n",
    "RAG_EXPERIMENT_MODELS = [\n",
    "    \"GPT3.5Turbo\",\n",
    "    \"FineTunedGPT3.5Turbo\",\n",
    "    \"GPT4Turbo\",\n",
    "    \"Llama3 8B\",\n",
    "    \"Llama3.3 70B\",\n",
    "]\n",
    "TOKENIZER_MAP = {\n",
    "    \"GPT\": tiktoken.get_encoding(\"cl100k_base\").encode,\n",
    "    \"Llama3\": AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\").tokenize,\n",
    "}\n",
    "PLOT_DATA_PATH = \"plotdata\"\n",
    "TEST_SUITE_STATS_PATH = \"plotdata/test_suite_stats.jsonl\"\n",
    "COVERAGE_TIMING_PATH = \"plotdata/coverage_timing.csv\"\n",
    "MODEL_SECONDS_PATH = \"plotdata/model_seconds.json\"\n",
    "FILTERED_SCENARIOS_PATH = \"plotdata/filtered_scenarios.csv\"\n",
    "COUNT_METRICS_PATH = \"plotdata/count_metrics.csv\"\n",
    "LARGE_TEST_SUITE_PATH = \"plotdata/large_test_suite.csv\"\n",
    "LABELS = [\"similar\", \"fixed\", \"buggy\"]\n",
    "FORMAL_LABELS = [\"already-tested\", \"need-test\", \"error-prone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for plots\n",
    "DARK_GREEN = \"#1e8449\"\n",
    "DARK_BLUE = \"#2874a6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_scenarios = pd.read_csv(FILTERED_SCENARIOS_PATH)\n",
    "df_test_suite_stats = pd.read_json(TEST_SUITE_STATS_PATH, lines=True)\n",
    "df_test_suite_stats[\"bug\"] = df_test_suite_stats[\"bug\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompt Time\n",
    "- GPT models\n",
    "- GPT models with RAG\n",
    "- Llama3 models\n",
    "- Llama3 models with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rag_experiment_seconds(\n",
    "    experiment_path: str, df_filtered_scenarios: pd.DataFrame\n",
    ") -> list:\n",
    "    \"\"\"Read elapsed seconds for each scenario during the experiment.\"\"\"\n",
    "    experiment_seconds = []\n",
    "    for file in os.listdir(experiment_path):\n",
    "        if file.startswith(\"prompt_\") and file.endswith(\".jsonl\"):\n",
    "            match = re.match(r\"prompt_([a-zA-Z]+)_(\\d+).jsonl\", file)\n",
    "            cur_project = match.group(1)\n",
    "            cur_bug = int(match.group(2))\n",
    "            with open(os.path.join(experiment_path, file)) as f:\n",
    "                for line in f:\n",
    "                    prompt_detail = json.loads(line.strip())\n",
    "                    cur_scenario_index = prompt_detail[\"scenario_index\"]\n",
    "                    if df_filtered_scenarios.query(\n",
    "                        f\"(project == '{cur_project}') & (bug == {cur_bug}) & (scenario_index == {cur_scenario_index})\"\n",
    "                    ).empty:\n",
    "                        continue\n",
    "                    elapsed_seconds = (\n",
    "                        sum(\n",
    "                            [q[\"elapsed_nanoseconds\"] for q in prompt_detail[\"queries\"]]\n",
    "                        )\n",
    "                        / 1_000_000_000\n",
    "                    )\n",
    "                    experiment_seconds.append(elapsed_seconds)\n",
    "    return experiment_seconds\n",
    "\n",
    "\n",
    "def read_time_raw_models() -> dict:\n",
    "    # Read elapsed seconds for each LLM\n",
    "    model_seconds = {}\n",
    "    for model in RAW_EXPERIMENT_MODELS:\n",
    "        model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"vanilla\", model)\n",
    "        experiment_seconds = []\n",
    "        for experiment in os.listdir(model_path):\n",
    "            experiment_path = os.path.join(model_path, experiment)\n",
    "            # Skip not directories\n",
    "            if not os.path.isdir(experiment_path):\n",
    "                continue\n",
    "            df_stats = pd.read_csv(os.path.join(experiment_path, \"statistics.csv\"))\n",
    "            df_stats[\"elapsed_seconds\"] = (\n",
    "                df_stats[\"elapsed_nanoseconds\"] / 1_000_000_000\n",
    "            )\n",
    "            experiment_seconds += df_stats[\"elapsed_seconds\"].to_list()\n",
    "        model_seconds[model] = experiment_seconds\n",
    "    return model_seconds\n",
    "\n",
    "\n",
    "def read_time_rag_models(df_filtered_scenarios: pd.DataFrame) -> dict:\n",
    "    # Read elapsed seconds for each LLM\n",
    "    model_seconds = {}\n",
    "    for model in RAG_EXPERIMENT_MODELS:\n",
    "        model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"rag\", model)\n",
    "        experiment_seconds = []\n",
    "        for experiment in os.listdir(model_path):\n",
    "            experiment_path = os.path.join(model_path, experiment)\n",
    "            # Skip not directories\n",
    "            if not os.path.isdir(experiment_path):\n",
    "                continue\n",
    "            experiment_seconds += read_rag_experiment_seconds(\n",
    "                experiment_path, df_filtered_scenarios\n",
    "            )\n",
    "        model_seconds[model] = experiment_seconds\n",
    "    return model_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coverage_timing = pd.read_csv(COVERAGE_TIMING_PATH)\n",
    "df_coverage_timing[\"elapsed_seconds\"] = (\n",
    "    df_coverage_timing[\"elapsed_nanoseconds\"] / 1_000_000_000\n",
    ")\n",
    "df_coverage_timing[\"elapsed_seconds\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_SECONDS_PATH):\n",
    "    with open(MODEL_SECONDS_PATH) as f:\n",
    "        model_seconds = json.load(f)\n",
    "        raw_model_seconds = model_seconds[\"raw_model_seconds\"]\n",
    "        rag_model_seconds = model_seconds[\"rag_model_seconds\"]\n",
    "else:\n",
    "    raw_model_seconds = read_time_raw_models()\n",
    "    rag_model_seconds = read_time_rag_models(df_filtered_scenarios)\n",
    "    # Save plot data if not exists\n",
    "    with open(MODEL_SECONDS_PATH, \"w\") as f:\n",
    "        model_seconds = {\n",
    "            \"raw_model_seconds\": raw_model_seconds,\n",
    "            \"rag_model_seconds\": rag_model_seconds,\n",
    "        }\n",
    "        json.dump(model_seconds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_time_boxplot_with_coverage(\n",
    "    comparison_pairs: list,\n",
    "    raw_model_seconds: dict,\n",
    "    rag_model_seconds: dict,\n",
    "    elapsed_seconds: list,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), tight_layout=True)\n",
    "    numbersize = 21\n",
    "    textsize = 22\n",
    "    raw_seconds = [raw_model_seconds[raw_model] for raw_model, _, _ in comparison_pairs]\n",
    "    rag_seconds = [rag_model_seconds[rag_model] for _, rag_model, _ in comparison_pairs]\n",
    "\n",
    "    raw_medians = [np.median(data) for data in raw_seconds]\n",
    "    rag_medians = [np.median(data) for data in rag_seconds]\n",
    "    width = 0.4\n",
    "    positions = np.arange(len(comparison_pairs) + 1) * 2\n",
    "    distance = 0.9 * width\n",
    "    raw_positions = positions[:-1] - distance\n",
    "    rag_positions = positions[:-1] + distance\n",
    "    raw_boxplot = ax.boxplot(\n",
    "        raw_seconds,\n",
    "        positions=raw_positions,\n",
    "        widths=width,\n",
    "        showfliers=False,\n",
    "        label=\"prompt without RAG\",\n",
    "        patch_artist=True,\n",
    "    )\n",
    "    for patch in raw_boxplot[\"boxes\"]:\n",
    "        patch.set_facecolor(DARK_GREEN)\n",
    "    rag_boxplot = ax.boxplot(\n",
    "        rag_seconds,\n",
    "        positions=rag_positions,\n",
    "        widths=width,\n",
    "        showfliers=False,\n",
    "        label=\"prompt with RAG\",\n",
    "        patch_artist=True,\n",
    "    )\n",
    "    for patch in rag_boxplot[\"boxes\"]:\n",
    "        patch.set_facecolor(DARK_BLUE)\n",
    "\n",
    "    cov_boxplot = ax.boxplot(\n",
    "        elapsed_seconds,\n",
    "        positions=[positions[-1]],\n",
    "        widths=width,\n",
    "        showfliers=False,\n",
    "        patch_artist=True,\n",
    "    )\n",
    "    for patch in cov_boxplot[\"boxes\"]:\n",
    "        patch.set_facecolor(\"tab:brown\")\n",
    "\n",
    "    # Add text annotation for the median value\n",
    "    for i, median in enumerate(raw_medians):\n",
    "        line = raw_boxplot[\"whiskers\"][2 * i + 1]\n",
    "        print(\n",
    "            \"RAW\",\n",
    "            raw_boxplot[\"whiskers\"][2 * i].get_ydata(),\n",
    "            raw_boxplot[\"whiskers\"][2 * i + 1].get_ydata(),\n",
    "        )\n",
    "        ax.text(\n",
    "            line.get_xdata()[-1],\n",
    "            line.get_ydata()[-1],\n",
    "            f\"{median:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=numbersize,\n",
    "        )\n",
    "    for i, median in enumerate(rag_medians):\n",
    "        line = rag_boxplot[\"whiskers\"][2 * i + 1]\n",
    "        print(\n",
    "            \"RAG\",\n",
    "            rag_boxplot[\"whiskers\"][2 * i].get_ydata(),\n",
    "            rag_boxplot[\"whiskers\"][2 * i + 1].get_ydata(),\n",
    "        )\n",
    "        ax.text(\n",
    "            line.get_xdata()[-1],\n",
    "            line.get_ydata()[-1],\n",
    "            f\"{median:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=numbersize,\n",
    "        )\n",
    "\n",
    "    cov_line = cov_boxplot[\"whiskers\"][1]\n",
    "    cov_median = np.median(elapsed_seconds)\n",
    "    print(\n",
    "        \"Coverage\",\n",
    "        cov_boxplot[\"whiskers\"][0].get_ydata(),\n",
    "        cov_boxplot[\"whiskers\"][1].get_ydata(),\n",
    "    )\n",
    "    ax.text(\n",
    "        cov_line.get_xdata()[-1],\n",
    "        cov_line.get_ydata()[-1],\n",
    "        f\"{cov_median:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=numbersize,\n",
    "    )\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(fontsize=textsize)\n",
    "    labels = [model for _, _, model in comparison_pairs] + [\"Coverage\"]\n",
    "    ax.set_xticks(positions)  # Center the ticks between the two boxes\n",
    "    ax.set_xticklabels(labels, rotation=35, ha=\"right\", fontsize=textsize)\n",
    "    # ax.set_xlabel(\"Model\", fontsize=textsize)\n",
    "    ax.set_ylabel(\"Time (seconds)\", fontsize=textsize)\n",
    "    ax.tick_params(axis=\"y\", labelsize=textsize)\n",
    "    ax.set_ylim(top=cov_line.get_ydata()[-1] + 20)\n",
    "    fig.savefig(\"images/time_boxplot.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_pairs = [\n",
    "    (\"GPT3.5Turbo\", \"GPT3.5Turbo\", \"GPT-3.5 Turbo\"),\n",
    "    (\"FineTunedGPT3.5Turbo\", \"FineTunedGPT3.5Turbo\", r\"\\textbf{\\textsc{E-Test}}\"),\n",
    "    (\"GPT4Turbo\", \"GPT4Turbo\", \"GPT-4 Turbo\"),\n",
    "    (\"Llama3 8B\", \"Llama3 8B\", \"Llama3 8B\"),\n",
    "    (\"Llama3 70B\", \"Llama3.3 70B\", \"Llama3 70B\"),\n",
    "]\n",
    "draw_time_boxplot_with_coverage(\n",
    "    comparison_pairs,\n",
    "    raw_model_seconds,\n",
    "    rag_model_seconds,\n",
    "    df_coverage_timing[\"elapsed_seconds\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Index Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_stats_dataframe(df_count_metrics: pd.DataFrame):\n",
    "    stats = {\"index_seconds\": [], \"num_loc\": []}\n",
    "    for model in RAG_EXPERIMENT_MODELS:\n",
    "        if model == \"Llama3.3 70B\":\n",
    "            model = \"Llama3 70B\"\n",
    "        model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"rag\", model)\n",
    "        for experiment in os.listdir(model_path):\n",
    "            experiment_path = os.path.join(model_path, experiment)\n",
    "            # Skip not directories\n",
    "            if not os.path.isdir(experiment_path):\n",
    "                continue\n",
    "            target_scenario = experiment.split(\"_\")[-1].upper()\n",
    "            version = \"b\" if target_scenario in [\"BUGGY\", \"SIMILAR\"] else \"f\"\n",
    "            summary_path = os.path.join(experiment_path, \"summary.jsonl\")\n",
    "            with open(summary_path) as f:\n",
    "                for line in f:\n",
    "                    prompt_detail = json.loads(line.strip())\n",
    "                    project = prompt_detail[\"project\"]\n",
    "                    bug = int(prompt_detail[\"bug\"])\n",
    "                    # Read index seconds and number of lines of code\n",
    "                    index_seconds = prompt_detail[\"index_nanoseconds\"] / 1_000_000_000\n",
    "                    num_loc = df_count_metrics.query(\n",
    "                        f\"(project == '{project}') & (bug == {bug}) & (version == '{version}')\"\n",
    "                    )[\"num_lines\"].iloc[0]\n",
    "                    # Save stats\n",
    "                    stats[\"index_seconds\"].append(index_seconds)\n",
    "                    stats[\"num_loc\"].append(num_loc)\n",
    "    return pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_metrics = pd.read_csv(COUNT_METRICS_PATH)\n",
    "df_count_metrics[\"bug\"] = df_count_metrics[\"bug\"].astype(int)\n",
    "df_index_stats = create_index_stats_dataframe(df_count_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_seconds_per_kline = (\n",
    "    sum(df_index_stats[\"index_seconds\"]) / sum(df_index_stats[\"num_loc\"]) * 1000\n",
    ")\n",
    "print(\n",
    "    f\"Average seconds spent by RAG indexing per 1,000 lines of code: {avg_seconds_per_kline:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumed Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_consumed_tokens() -> pd.DataFrame:\n",
    "    \"\"\"Read consumed tokens using raw prompts into a DataFrame with columns [tokenizer, num_tokens].\"\"\"\n",
    "    consumed_tokens = {\"tokenizer\": [], \"num_tokens\": []}\n",
    "    for model in RAW_EXPERIMENT_MODELS:\n",
    "        if model == \"FineTunedGPT3.5Turbo\":\n",
    "            model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"finetuning\", model)\n",
    "        else:\n",
    "            model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"vanilla\", model)\n",
    "        for experiment in os.listdir(model_path):\n",
    "            experiment_path = os.path.join(model_path, experiment)\n",
    "            # Skip not directories\n",
    "            if not os.path.isdir(experiment_path):\n",
    "                continue\n",
    "            # Determine tokenizer\n",
    "            if \"GPT\" in model:\n",
    "                tokenizer = \"GPT\"\n",
    "            elif \"Llama3\" in model:\n",
    "                tokenizer = \"Llama3\"\n",
    "            else:\n",
    "                raise ValueError(f\"Fail to determine tokenizer of model {model}!\")\n",
    "            df_stats = pd.read_csv(os.path.join(experiment_path, \"statistics.csv\"))\n",
    "            df_stats[\"tokenizer\"] = tokenizer\n",
    "            consumed_tokens[\"tokenizer\"] += df_stats[\"tokenizer\"].to_list()\n",
    "            consumed_tokens[\"num_tokens\"] += df_stats[\"#tokens\"].to_list()\n",
    "    return pd.DataFrame(consumed_tokens)\n",
    "\n",
    "\n",
    "def compute_rag_jsonl_tokens(\n",
    "    experiment_path: str, df_filtered_scenarios: pd.DataFrame\n",
    ") -> list:\n",
    "    \"\"\"Compute number of tokens for each RAG prompt scenario and return a list of {\"tokenizer\": xxx, \"num_tokens_per_query\": xxx, \"num_tokens\": xxx}.\"\"\"\n",
    "    consumed_tokens = []\n",
    "    target_scenario = experiment_path.split(\"_\")[-1].upper()\n",
    "    # Determine tokenizer\n",
    "    if \"GPT\" in experiment_path:\n",
    "        tokenizer_name = \"GPT\"\n",
    "        tokenizer = TOKENIZER_MAP[\"GPT\"]\n",
    "    elif \"Llama3\" in experiment_path:\n",
    "        tokenizer_name = \"Llama3\"\n",
    "        tokenizer = TOKENIZER_MAP[\"Llama3\"]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Fail to determine tokenizer of experiment at {experiment_path}!\"\n",
    "        )\n",
    "    # Iterate over each prompt detail file\n",
    "    for file in os.listdir(experiment_path):\n",
    "        if file.startswith(\"prompt_\") and file.endswith(\".jsonl\"):\n",
    "            match = re.match(r\"prompt_([a-zA-Z]+)_(\\d+).jsonl\", file)\n",
    "            cur_project = match.group(1)\n",
    "            cur_bug = int(match.group(2))\n",
    "            # Read jsonline file line by line\n",
    "            with open(os.path.join(experiment_path, file)) as f:\n",
    "                for line in f:\n",
    "                    prompt_detail = json.loads(line.strip())\n",
    "                    cur_scenario_index = prompt_detail[\"scenario_index\"]\n",
    "                    # Filter scenarios\n",
    "                    if (\n",
    "                        target_scenario != \"SIMILAR\"\n",
    "                        and df_filtered_scenarios.query(\n",
    "                            f\"(project == '{cur_project}') & (bug == {cur_bug}) & (scenario_index == {cur_scenario_index})\"\n",
    "                        ).empty\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Compute number of tokens using tokenizer\n",
    "                    num_tokens_per_query = []\n",
    "                    for query_item in prompt_detail[\"queries\"]:\n",
    "                        num_tokens = len(tokenizer(query_item[\"prompt_str\"]))\n",
    "                        num_tokens_per_query.append(num_tokens)\n",
    "                    consumed_tokens.append(\n",
    "                        {\n",
    "                            \"tokenizer\": tokenizer_name,\n",
    "                            \"num_tokens_per_query\": num_tokens_per_query,\n",
    "                            \"num_tokens\": sum(num_tokens_per_query),\n",
    "                        }\n",
    "                    )\n",
    "    return consumed_tokens\n",
    "\n",
    "\n",
    "def read_rag_consumed_tokens(df_filtered_scenarios: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Read consumed tokens using RAG prompts into a DataFrame with columns [tokenizer, num_tokens].\"\"\"\n",
    "    consumed_tokens = []\n",
    "    for model in RAG_EXPERIMENT_MODELS:\n",
    "        if model == \"Llama3.3 70B\":\n",
    "            model = \"Llama3 70B\"\n",
    "        model_path = os.path.join(EXPERIMENT_RESULTS_PATH, \"rag\", model)\n",
    "        for experiment in os.listdir(model_path):\n",
    "            print(f\"Counting tokens using model {model} in experiment {experiment}\")\n",
    "            experiment_path = os.path.join(model_path, experiment)\n",
    "            # Skip not directories\n",
    "            if not os.path.isdir(experiment_path):\n",
    "                continue\n",
    "            consumed_tokens += compute_rag_jsonl_tokens(\n",
    "                experiment_path, df_filtered_scenarios\n",
    "            )\n",
    "    return pd.DataFrame(consumed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_tokens = read_raw_consumed_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rag_tokens = read_rag_consumed_tokens(df_filtered_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_tokens.to_csv(os.path.join(PLOT_DATA_PATH, \"raw_tokens.csv\"), index=False)\n",
    "# df_rag_tokens.to_csv(os.path.join(PLOT_DATA_PATH, \"rag_tokens.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_tokens = pd.read_csv(os.path.join(PLOT_DATA_PATH, \"raw_tokens.csv\"))\n",
    "df_rag_tokens = pd.read_csv(\n",
    "    os.path.join(PLOT_DATA_PATH, \"rag_tokens.csv\"),\n",
    "    converters={\"num_tokens_per_query\": literal_eval},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_consumed_tokens_boxplot(\n",
    "    df_raw_tokens: pd.DataFrame, df_rag_tokens: pd.DataFrame\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4), tight_layout=True)\n",
    "    numbersize = 21\n",
    "    fontsize = 22\n",
    "    raw_data = [\n",
    "        df_raw_tokens.query(f\"tokenizer == '{tokenizer}'\")[\"num_tokens\"].values\n",
    "        for tokenizer in TOKENIZER_MAP.keys()\n",
    "    ]\n",
    "    rag_data = [\n",
    "        df_rag_tokens.query(f\"tokenizer == '{tokenizer}'\")[\"num_tokens\"].values\n",
    "        for tokenizer in TOKENIZER_MAP.keys()\n",
    "    ]\n",
    "    rag_per_query_data = [\n",
    "        [\n",
    "            v\n",
    "            for values in df_rag_tokens.query(f\"tokenizer == '{tokenizer}'\")[\n",
    "                \"num_tokens_per_query\"\n",
    "            ].values\n",
    "            for v in values\n",
    "        ]\n",
    "        for tokenizer in TOKENIZER_MAP.keys()\n",
    "    ]\n",
    "\n",
    "    raw_medians = [np.median(data) for data in raw_data]\n",
    "    rag_medians = [np.median(data) for data in rag_data]\n",
    "    rag_per_query_medians = [np.median(data) for data in rag_per_query_data]\n",
    "\n",
    "    width = 0.2\n",
    "    positions = np.arange(len(TOKENIZER_MAP.keys())) * 3.7\n",
    "    distance = 1.5 * width\n",
    "\n",
    "    raw_positions = positions - distance\n",
    "    rag_per_query_positions = positions\n",
    "    rag_positions = positions + distance\n",
    "\n",
    "    for data, pos, label, color, medians in [\n",
    "        (raw_data, raw_positions, \"prompt without RAG\", DARK_GREEN, raw_medians),\n",
    "        (\n",
    "            rag_per_query_data,\n",
    "            rag_per_query_positions,\n",
    "            \"each query with RAG\",\n",
    "            \"purple\",\n",
    "            rag_per_query_medians,\n",
    "        ),\n",
    "        (rag_data, rag_positions, \"all queries with RAG\", DARK_BLUE, rag_medians),\n",
    "    ]:\n",
    "        boxplot = ax.boxplot(\n",
    "            data,\n",
    "            positions=pos,\n",
    "            widths=width,\n",
    "            showfliers=False,\n",
    "            label=label,\n",
    "            patch_artist=True,\n",
    "        )\n",
    "        for patch in boxplot[\"boxes\"]:\n",
    "            patch.set_facecolor(color)\n",
    "        # Add text annotation for the median value\n",
    "        for i, median in enumerate(medians):\n",
    "            line = boxplot[\"whiskers\"][2 * i + 1]\n",
    "            ax.text(\n",
    "                line.get_xdata()[-1],\n",
    "                line.get_ydata()[-1] + 350,\n",
    "                f\"{median:.0f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=numbersize,\n",
    "            )\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(0.885, 1), fontsize=22)\n",
    "    ax.set_xticks(positions)  # Center the ticks between the two boxes\n",
    "    ax.set_xticklabels(list(TOKENIZER_MAP.keys()))\n",
    "    # ax.set_xlabel(\"Tokenizer\", fontsize=fontsize)\n",
    "    ax.set_ylabel(\"Consumed \\#Tokens\", fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", labelsize=fontsize)\n",
    "    ax.set_ylim(0, 13500)\n",
    "    fig.savefig(\"images/consumed_tokens_boxplot.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_consumed_tokens_boxplot(df_raw_tokens, df_rag_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
